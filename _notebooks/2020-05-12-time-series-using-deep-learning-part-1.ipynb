{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Time Series Classification Using Deep Learning - Part 1\n",
    "> A soft introduction to time series classification using state of the art neural network architecture.\n",
    "\n",
    "- toc: true \n",
    "- badges: false\n",
    "- comments: true\n",
    "- categories: [AI]\n",
    "- author: Farid Hassainia, PhD"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this article, I will introduce you to a new package called [timeseries](https://github.com/ai-fast-track/timeseries)  for [fastai2](https://github.com/fastai/fastai2) that I lately developed. The timeseries package allows you to train a Neural Network (NN) model in order to classify both univariate and multivariate time series using the powerful fastai2 library and achieve a State Of The Art (SOTA) results. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Objectives:\n",
    "The key objectives of this series of articles are:\n",
    "\n",
    "1. Introduce you to time series classification using Deep Learning,\n",
    "\n",
    "2. Show you a step by step how this package was built using fastai2 library,\n",
    "\n",
    "3. Introduce you to some key concepts of the fastai2 library such as Datasets, DataLoaders, DataBlock, Transform, etc.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "Unlike Computer Vision (CV), Time Series (TS) analysis is not the hottest topic in the Artificial Intelligence (AI)/DL eco-system. Lately, CV starts to lose a bit of its luster because of the amount of controversies around facial detection abusive applications among other reasons. As a potential positive consequence, we hope more attention will be geared towards TS analysis opening the door to more innovation in the TS field.\n",
    "\n",
    "Another positive aspect of using DL in the TS field is the fact that training a NN TS model uses far less both CPU and GPU resources. As a matter of fact, it uses a fraction of the compute resources in comparison to those needed to train NN models in both CV and NLP. \n",
    "\n",
    "Compared to both CV and Natural Language Processing (NLP), TS is still in its infancy in terms of DL innovations. This offers the opportunity to talented people to step in this domain and to start innovating and creating new models that are specifically designed to time series. Those new models could be built from the ground or inspired by some well established NN models such as ResNet in CV, and LSTM in NLP. \n",
    " \n",
    "fastai2 could play a significant role in developing this field thanks to its unified APIs that already spans several domains such as Vison, NLP, and Tabular, on one hand, and to its level of granularity in terms of APIs depth (High, Mid, and Low Levels APIs), on the other hand. \n",
    " \n",
    "By using fastai2, not only, this will accelerate the development of new libraries but it will also offer a fast learning curve for users exploring those new TS libraries: in other word, we can leverage the transfer learning between different modules that constitutes the fastai2 library.\n",
    "\n",
    "Before diving in the core subject, let's define what is a time series.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Short Introduction\n",
    "A time series is a simple set of data point stored in a chronological order (time stamps). Time series can be found in virtually all domains and can be divided into 2 types:\n",
    "\n",
    "•\tUnivariate time series: Representing a single wave (e.g. ECG recording)\n",
    "\n",
    "![](images/ECG.png)\n",
    "\n",
    "•\tMultivariate time series: Representing a set of related waves such as NATOPS recording which represent sensors recordings at different location (see here below for more details)\n",
    "\n",
    "![](images/NATOPS-24-Channels.png)\n",
    "\n",
    "![](images/NATOPS.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Time Series Analysis\n",
    "Time series analysis is becoming increasingly important because of the easy access and the wide spread of time series generation tool such as mobile devices, internet of things, logging and monitoring data generated in virtually all the industries, etc. What will become even more crucial is the design of end-to-end tools capable of analyzing such a gigantic amount of data. Deep Learning will play a key role in providing such tools.\n",
    " \n",
    "Time series analysis aims to extract meaningful information that allow the user to get actionable insights. It can be divided into 3 main categories:\n",
    " \n",
    "1.\tTime Series Classification (TSC): We present a time series to a Neural Network (NN) model, and the latter predicts its label (class). This article will focus on this category\n",
    "2.\tTime Series Regression (TSR): This is quite similar to TSC, and share the same data processing. It can be seen as TSC special case where the number of labels (classes) is reduced to 1, and represented by a float instead of an integer (category)\n",
    "3.\tTime Series Forecasting (TSF):  it consists in predicting the future values (or range of values) of a time series (e.g. temperature, sales, stock price, etc.) based on previously observed values\n",
    " \n",
    "The timeseries package presented in this article covers both time series classification and regression."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: AIoT and AI in Edgepoint \n",
    "Many big companies and startups are heavily investing in developing both AI-enabled processors and micro-controllers for AI in edgepoint Applications. \n",
    "\n",
    ">Nowadays, the edge AI portion of a smartphone System-on-Chip (SoC) represents only about 5% of the total area and about US$3.50 of the total cost, and would use about 95% less power than the whole SoC does. This stressed out how affordable the AI chips already are. \n",
    "\n",
    ">The wide spread of both AI chips and artificial intelligence of things (AIoT) will make NN model training and/or inference at edgepoints a reality. AI inference on the edgepoint device is very attractive because it increases latency, saves bandwidth, helps privacy, and saves power associated with RF transmission of data to the cloud. Furthermore, the ability to collect, interpret, and immediately act on vast amounts of data is critical for many of the data-heavy applications.\n",
    "\n",
    ">As a consequence, this will likely drive significant changes for consumers and enterprises applications. Smart machines powered by AI chips will have a huge impact on a wide variety of industries such as manufacturing, logistics, agriculture, energy, etc. \n",
    "\n",
    ">TS processing using AI/DL techniques could play a key role in implementing AI on edgepoint given the fact that TS uses far less resources such as memory and compute.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Toy Data\n",
    "As an illustration, I will use a multivariate time series from Naval Air Training and Operating Procedures Standardization (NATOPS) dataset. The data is generated by sensors on the hands, elbows, wrists and thumbs (see figure here above). The data are the x, y, z coordinates for each of the 8 locations. For each sensor and each axis, we have a time series that represent the value of x (respectively y, and z) during the execution of a command. For instance, channel 3 (ch3) on the graph, here above, shows the Hand tip right X coordinate at different timestamps. For each gesture, we will have 24 curves corresponding to 8 sensors x 3 coordinates. The whole represents a multivariate time series.\n",
    "\n",
    "### Classes (Labels)\n",
    "The dataset contains six classes representing 6 separate actions, with the following meaning:\n",
    "1: I have command 2: All clear 3: Not clear 4: Spread wings 5: Fold wings 6: Lock wings\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## What is the goal of the NATOPS time series classification?\n",
    "The NATOPS dataset contains time series recordings corresponding to different commands executed by different operators. \n",
    " \n",
    "The goal is to train our NN model using both a training dataset and a valid dataset. After training our model and achieving a high accuracy score, we feed our model a given sensor test data without providing its class (label) (e.g. \"4: Spread wings\"), and our model will predict which class the sensor data correspond to  (i.e. \"4: Spread wings\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Training\n",
    "In this section, I will show how easy to train a NN model, and achieve some SOTA results in a record time. Before doing that, let’s introduce the timeseries package. The latter was designed to mimic the unified fastai v2 APIs used for vision, text, and tabular in order to ease its use among users familiar with the fastai2 APIs. As a consequence, those who already used fastai2 vision module will feel familiar with the timeseries APIs. Timeseries package uses Datasets, DataBlock, and a new TSDataLoaders and a new TensorTS classes. It has the following mapping with fastai2 vision module:\n",
    " \n",
    "TensorImage  <---> TensorTS\n",
    "Conv2D       <---> Conv1D\n",
    " \n",
    "The timeseries package also references 128 Univariate and 30 Multivariate time series datasets. Using URLs_TS class (similar to fastai URLs class) you might play with one of those 158 datasets.\n",
    "Similarly, to fastai vision examples, we can train any time series dataset end-to-end with 4 lines of code. In the example shown, here below, we use TSDataLoaders and a multivariate the NATOPS dataset. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You can run the whole [training_using_default_settings.ipynb](https://github.com/ai-fast-track/timeseries/blob/master/nbs/training_using_default_settings.ipynb) notebook in Google Colab by clicking on the link."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "path = unzip_data(URLs_TS.NATOPS)\n",
    "dls = TSDataLoaders.from_files(bs=32,fnames=[path/'NATOPS_TRAIN.arff', path/'NATOPS_TEST.arff'], batch_tfms=[Normalize()]) \n",
    "learn = ts_learner(dls)\n",
    "learn.fit_one_cycle(25, lr_max=1e-3) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using a SOTA NN model called [InceptionTime](https://arxiv.org/pdf/1909.04939.pdf) (published in September 2019), and the default fastai2 settings, we can achieve around 98,5% accuracy in only 20 epochs. The following figure shows some of the predictions results (Predicted/True classes): "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/show-results.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The package also features Class Activation Map (CAM) for time series. CAM is used to help interpreting the results of our NN model decision. The package offers both CAM and GRAD-CAM as well as user-defined CAM.  The [cam_tutorial_ECG200.ipynb](https://github.com/ai-fast-track/timeseries/blob/master/nbs/cam_tutorial_ECG200.ipynb) notebook illustrates a simple example of the univariate ECG200 dataset classification task (Normal Heartbeat vs. Myocardial Infarction). Like in vision, the colors represent the activation values at a given layer (in this example it is located before the FC layer (last layer)). Notice how the Myocardial Infarction plots (2nd, 3rd, and 4th) share similar activation zones that are quite different from those corresponding to Normal Heartbeat plots (1st and 5th). This kind of representation eases the interpretation of the results obtained using a given NN model (InceptionTime, in this case)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    ">Note: You can also run the The [cam_tutorial_ECG200.ipynb](https://github.com/ai-fast-track/timeseries/blob/master/nbs/cam_tutorial_ECG200.ipynb) notebook in Google Colab by clicking on the link."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/CAM-ECG.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Code Walk Through\n",
    "let’s decipher, line by line, the 4 lines of code shown here above:\n",
    " \n",
    "1. In line 1, we are downloading a dataset (NATOPS dataset precisely) that is hosted at the [Time Series Classification Repository website]( http://www.timeseriesclassification.com/), unzipping the dataset, and saving it a separate folder under the local ./fastai/data folder. \n",
    " \n",
    "2. In line 2, a lot of action is occurring there, and can be summarized as follow:\n",
    "- Create a 2 Dataset objects containing a train dataset, and a valid dataset.\n",
    " \n",
    "- Create 2 DataLoader objects that allows us to create mini-batches for both training and valid datasets. \n",
    " \n",
    "3. In line 3, we create a learner where we basically do the following:\n",
    "- Create an NN model, InceptionTime in our case. \n",
    " \n",
    "- Create a Learner object using some state-of-the-art techniques found in the fastai2 library such as the Ranger optimizer, callbacks, etc.\n",
    " \n",
    "4. At line 4, we train our model using one the fastai secret ingredient being the fast converging training algorithm called fit_one_cycle(). Running the last line, we achieve accuracy higher than 98% in less than 20 epochs.\n",
    " \n",
    "Admittedly, the 4 lines shown here above can be a bit cryptic for someone how is new to the fastai2 library. Those lines show how to train a model using the fastai2 high level API. At that level, many settings are set by default in order to avoid overwhelming new users by exposing too many parameters that are difficult to grasp at the beginning of their learning journey. As shown above, using fastai2 default settings we are very often capable of reaching SOTA results in fewer epochs. \n",
    "\n",
    "As a very versatile API covering a large range of use-cases, fastai2 offers 3 levels of APIs: High level, Mid Level, and Low Level. Depending on both use-cases and user's fastai2 proficiency, one might choose one or the other level.\n",
    " \n",
    "In the next article, I will take that opportunity to start building the timeseries module using the Mid Level APIs. I chose that level because both fastai version 1 and/or Pytorch users will feel in familiar territories. We will leverage the new fastai2 Datasets and DataLoaders classes, and show how both versatile and powerful are."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "I hope this first article convey you into trying the timeseries package. You might also check out its [documentation](https://ai-fast-track.github.io/timeseries/). All the notebooks are self-contained, and documented. You will be able to directly run them in Google Colab.  \n",
    " \n",
    "If you give it a try and found it interesting/helpful, please let others know it by staring it on GitHub, and share with your friends and colleagues who might be interested in this topic.\n",
    "\n",
    "As a new Twitter user, I would like to kindly ask you to follow me @ai_fast_track. I will post there the sequel of this blog post.\n",
    "\n",
    "I also intend to post on Twitter, on regular basis, Tips & Tricks about fastai(2) and Deep Learning in general. Stay Tuned, and Stay Safe!\n",
    "\n",
    ">Note: I wanted to end this article by saying that I was immune to \"unfollowers\" because I haven't any follower yet. To my big surprise, I discovered that I already had one follower: @jcatanz, before even starting tweeting. Thank you so much Joseph for being my first follower! Hopefully, other fastai members will do the same!\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](images/tree.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.10-final"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}