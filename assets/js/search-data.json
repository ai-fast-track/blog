{
  
    
        "post0": {
            "title": "3 ways to pip install a package - fastai2 use-case",
            "content": "Introduction . In this article, I will show 3 methods to install the new fastai2 library using pip install. The 3 methods described here can be used to similar python libraries found on GitHub. I voluntarily omitted the conda install option because I wanted too focus on methods that can be reproduced in free cloud environment such as Google Colab. . . Note: In this article, the different methods described are generic and therefore can be applied to any python package published on Github repository. You only have to replace the name fastai2 by the name of the corresponding python package (for example torch) . Method 1 . . Note: The first method is the most common way to install a stable version of a python package. It will be widely used once fastai2 will be officially released. . Although, fastai2 is still under development and its official release is expected this summer, it is important to be familiar with the present method especially if you will be using it in a production setting. It is also common to freeze install a package: installing a specific version a package (e.g pip install torch== 1.4.0). . To install fastai2 from [fastai2 pypi repository] (https://pypi.org/project/fastai2/), type the following command either from: . the terminal: . pip install fastai2 . | or from a Jupyter notebook cell: . !pip install fastai2 . | The difference between the 2 options is the use of the ! in the case of Jupyter notebook. However, the result is the same, the fastai2 package will be install in the current active environment. . . Important: Activate the virtual environment of your choice before installing any python package. Lacking to do so, the package will be installed your current virtual environment. This means that most likely, it will be installed in the base virtual environment in Ananconda (base is the default virual environment created when you installed Anaconda) . To check where the fastai2 package has been install, run the following command: . pip show fastai2 . This command displays some useful information: . Name: fastai2 Version: 0.0.11 Summary: Version 2 of the fastai library Home-page: https://github.com/fastai/fastai2 Author: Jeremy Howard, Sylvain Gugger, and contributors Author-email: info@fast.ai License: Apache Software License 2.0 Location: /home/farid/anaconda3/envs/fastai2/lib/python3.6/site-packages Requires: scikit-learn, pandas, fastprogress, requests, scipy, pillow, torchvision, matplotlib, pyyaml, spacy, torch, fastcore Required-by: . It displays the package name, version number, location where it was installed (with the name of the virtual environment: fastai2 is the name in my case), and other information. . Note: As of today (March 16th 2020), the fastai version found on the pypi repository is 0.0.11 . Method 2 . . Note: This method is used by users who are interested in new using the latest and the greatest features found the fastai2 Github repository. In general, they are interested in using the latest fastai2 package without the intention to build new modules that can be used as fastai2 extensions. If this sound a bit unclear, rest assured that everything will be crystal clear by the end of reading Method 3. . The second method consists in installing the fastai2 package from the fastai2 GitHub repository. It begs the question: Why would we be interested in doing so. The answer is simple: If a python package is heavy development and changed in the source coder are introduced on a regular basis, chances are the package found on the pypi repository is lagging behind. In order to use the latest and the greatest version you can just install fastai2 from its GitHub repository. . To use a concrete example, at certain moment Jeremy shared a Kaggle kernel in order to show how to use a new developed module called medical. The latter was under development and was not available on the pypi repository. Therefore the solution was to advice the users to install the latest fastai2 version (that includes the medical module) by running the following command either on: . the terminal: . pip install git+https://github.com/fastai/fastai2.git . | or from a Jupyter notebook cell: . !pip install git+https://github.com/fastai/fastai2.git . | You might have already noticed that we replaced fastai2 by git+https://github.com/fastai/fastai2.git which means that you are installing the latest source code found in the fastai2 GitHub repository. . Important: When using this option, it is very important to also install the fastcore library` using the very same method: from the command line: . pip install git+https://github.com/fastai/fastai2.git . and from a Jupyter notebook cell: . !pip install git+https://github.com/fastai/fastai2.git . You may ask yourself why do we have to also install the fastcore library from GitHub given the fact that fastai2 will always install fastcore because it is part of its requirements. The answer is because fastai2 will install the latest version found in the pypi repository and not the latest version found on GitHub. If the fastai github version is using the new capabilities found in the fastcore latest GitHub version. This will lead to an incompatibility if you are using fastai latest GitHub version mixed with the fastcore pypi version, you will end up have some issues and some errors will surface. Therefore, better to have the habit to pip install both of them using the same method (method 2 in this case) . Method 3 . . Note: This method is used by developers who are actively building new modules that depends on fastai2. These developers are usually actively adding features to their modules and want to make sure that their source code stay in sync with the fastai2 latest version. Lacking to do so, they will realize that their code is incompatible with fastai2 one. . The goal is to keep your library (which depends on fastai2) aligned to the fastai2 one hand, and to avoid to keep pip installing both fastai2 and fastcore every time new version are released (this would have been the case had we used Method 2). There exists a magic method (Method 3) that consists in using a so-called editable version of both fastai2 and fastcore. . Installing an editable version of fastai2 is pretty straightforward. All we have to do is to follow these 3 simple steps, on a terminal console: . git clone https://github.com/fastai/fastai2 cd fastai2 pip install -e . . Basically, we are cloning the fastai2 repository, then hopping into the fastai2 folder, finally issuing the command to pip install the package (fastai2) from the current folder (hence the use of the dot .) as an editable version by using the -e switch. . We also to follow the same steps for the fastcore library: . git clone https://github.com/fastai/fastcore cd fastcore pip install -e . . For those following along and who are familiar with this option, they might notice that there is a discrepancy between this method and the one found on the fastai2 repository: . git clone https://github.com/fastai/fastai2 cd fastai2 pip install -e &quot;.[dev]&quot; . Using the [dev] flag issue an order to the installer to pip install all the package listed under the dev requirements (dev_requirements = nbdev). This means the nbdev package (its a fastai package) will be installed from the pypi repository. Given the fact that nbdev is also under heavy development, we may experienced some issues caused by a misalignment between the 3 different fastai pacjkages being fastai2, fastcore, and nbdev. For that reason, it is desirable to also install the nbdev package as an editable package as follow: . git clone https://github.com/fastai/nbdev cd nbdev pip install -e . . Now that we installed our editable package, how are we going to update it. The answer is very straightforward, we pull our latest version from the corresponding GitHub repository. Therefore, for the fastai2 package, we just have to hop to the forder where we clone the original repo, and we run the following command: . git pull . This command will both update our repo and update our editable package has the version changed. We have to run the command, here above, for both fastai2 and fastcore. . Appendix A: How to install Anaconda . On linux: . $ curl -O https://repo.anaconda.com/archive/Anaconda3-2019.10-Linux-x86_64.sh $ sha256sum Anaconda3-2019.10-Linux-x86_64.sh 46d762284d252e51cd58a8ca6c8adc9da2eadc82c342927b2f66ed011d1d8b53 Anaconda3-2019.10-Linux-x86_64.sh $ bash Anaconda3-2019.10-Linux-x86_64.sh $ source ~/.bashrc . Youtube tutorial . Appendix B: How to create a conda virtual enviroment . On both Linux and Windows run the following commands: . conda create -n fastai2 python=3.6 anaconda conda activate fastai2 . . Tip: You can choose with version of python you would like to install. In the case above, the version 3.6 will be installed .",
            "url": "https://ai-fast-track.github.io/blog/python/2020/03/16/how-to-pip-install-package.html",
            "relUrl": "/python/2020/03/16/how-to-pip-install-package.html",
            "date": " • Mar 16, 2020"
        }
        
    
  
    
        ,"post1": {
            "title": "Time Series Classification Using Deep Learning - Part 1",
            "content": "Introduction . Unlike Computer Vision (CV), Time Series (TS) analysis is not the hottest topic, in the AI / Deep Learning world. Lately, CV starts to lose a bit of its luster because of the amount of controversies around facial detection abusive applications among other reasons. Meanwhile, one may hope that innovation in time series analysis will get more attention and more traction. Compared to both CV and NLP, there are not a lot of publications in time series analysis using Deep Learning. This open doors to talented people to stepping in this domain and start innovating and creating new models that are specially designed to time series. Those new models could be inspired by some already established Neural Network models such as ResNet in CV, and LSTM in NLP. . This brings us to fastai 2 library. The latter has a tremendous advantage in comparison to other DL frameworks by having a unified APIs that already spans several domains such as Vison, NLP, and Tabular. Therefore, it offers a singular opportunity for DL practitioners interested in creating complementary libraries that mimics the already well established fastai2 structure. By doing so, not only, it accelerates the development of new libraries but it also offers a fast learning curve for the users of those new extension libraries: in other word, we are taking advantage of the transfer learning between different modules that constitutes the fastai2 library. . Audience . This post is the part 1 of a 3-parts series that targets a large eclectic audience spanning from individuals having limited knowledge in deep learning to developers seeking to follow a walk through in developing a fastai2 extension using fastai2, fastcore,and nbdev libraries. . One of the goals of this series of posts is to break the fear of developing new modules, libraries, pieces of code some may feel starting playing with deep learning code and/or developing interesting things using the new fastai2. . What is in this series of articles? . First, let’s introduce the timeseries package: an unofficial extension for fastai version 2 (fastai2). This package is still under development (github) yet very functional and with a detailed documentation (docs). timeseries package focus on classification and regression tasks. . The 3-posts series can be presented as follow: . Part 1: it will introduce you to the 1) timeseries package, 2) Data loading, 3) Datasets, 4) DataLoaders, 5) InceptionTime model, and 5) train and validating the model. In part 1, we will learn how to create DataLoaders objects using Datasets classes. Datasets ar considered as mid-level APIs. . | Part 2: we will create DataLoaders objects using DataBlock objects which are considered as high-level APIs. . | Part 3: In this part, we will learn how to create TSDataloaders, a class that derives from fastai2 DataLoaders. Its purpose is to abstract the different steps found in using DataBlock class. We will also get deeper in the beast guts by exploring who the timeseries is internally structured. . | . What is a time series? . First, let’s introduce what time series are in order to level the ground for those who are familiar with time series analysis. For those familiar with this subject, they can skip the following part. . Time series data have a natural temporal ordering. This makes time series analysis distinct from tabular data (Spreadsheet-like data) in which there is no natural ordering of the observations (e.g. explaining people&#39;s wages by reference to their respective education levels, where the individuals&#39; data could be entered in any order). . Time series can divided in to 3 categories: . Time Series Classsification (TSC) : We present a time serie to a Neural Network (NN) model and the latter predicts its label (class). In this tutorial will be focused on this category . | Time Series Regression (TSR) : This is quiet similar to TSC and share the same data processing. It can be considered as specila case where the number of labels (classes) is reduced to 1, and represented by a float instead of a category. If it is confusing, rest assured we all had the same feeling at a given time of our learning journey. Every bit of information will become more clear as we advance in these tutorials: so hang on and we will reach our destination in a short time. . | Time series Annotation: It includes anomaly detection, segmentation . | Time series Forecasting: . | The fastai way . Following a long fastai tradition, I will show how we can train an NN model and reach a 98,6% accuracy in less than 20 epochs of a time series classification of one of the datasets used to benchmark different NN models. This high accuracy was considered as a state-of-the-art (SOTA) result just a couple of month ago. It is worthwhile to stress out that this result has been achieved by writing 4 lines of code: . path = unzip_data(URLs_TS.NATOPS) dls = TSDataLoaders.from_files(bs=32,fnames=[path/&#39;NATOPS_TRAIN.arff&#39;, path/&#39;NATOPS_TEST.arff&#39;], batch_tfms=[Normalize()]) learn = ts_learner(dls) learn.fit_one_cycle(25, lr_max=1e-3) . If you find it is too much magic for you, I woul agree with you if this the only way that we offer to train a model. Far from that, thanks to fastai2 APIs, a practitioner is able to decide what level (of magic) of the APIs to would like to operate at. It means they can use TfmLists, Datasets, DataBlock, and customized DataLoaders. If this is very cryptic for you, that is quite normal. Going through this series of blog posts, you will be grasp the underlying foundation of the APIs and become aquatinted with those concepts. So, hang on and your efforts will be rewarded very soon. . First, let’s decipher the code above line by line: . In line 1, we are downloading a dataset (NATOPS dataset precisely) that is hosted at by http://www.timeseriesclassification.com/ website (a Time Series Classification Repository), unzip the dataset, and save it a separate folder under the local ./fastai/data folder. In the case of NATOPS, we have 360 samples (It is small by the big data standard but we are using for testing purposes) . | In line 2, There is a lot of action there that can be summarized as follow: . | Create a 2 Dataset object containing a train dataset (288 samples), and a valid dataset (112 samples) | Create 2 DataLoader objects that allows us to create mini batches for each type of datasets. A batch is just a sample number of samples (e.g. 32 samples) | . In line 3, we create a learner where we basically do the following: | Create an NN model, InceptionTime (ref) in our case. The latter was published in September 2019, and achieved SOTA results. Oh did I tell that the researcher who introduced this new model is a member of the fastai community Hassan Fawaz? He also open sourced his Keras implementation. What is also amazing is that one of fastai member Ignacio Oguiza ported that code to fastai version 1, in record time and also open sourced it with many other NN models. | Create a Learner using some state-of-the-art as defaults such as the Ranger optimizer | . At line 4, we reach the high where we can train our model using one the fastai secret ingredient being the fast converging training algorithm called fit_one_cycle(). Running the last line, we achieve accuracy higher than 98% in less than 20 epochs, and this one of the reasons fastai has the adjective fast in its name, I guess. Now, that we known how to achieve a SOTA results using 4 lines, let’s further break down the explanation that I offered you here above. There many to tackle that by choosing which level should we use to explain to a new fastai2 comer how this machinery works. Different people would choose different level. My intuition consists in using the Datasets objects as the starting block. I hope it will transparently reveal you the different steps necessary in building datasets (train and valid), dataloaders in charge of creating the mini-batches that will feed our learner in order to train our NN model. | Example . In example, presented here below, we a multivariate time series. The data is generated by sensors on the hands, elbows, wrists and thumbs. The data are the x,y,z coordinates for each of the eight locations. For each sensor and each axis, we have a time serie that represent the value of x (or y, or z) during execution time of a command. For instance, channel 3 (ch3) on the graph shows the Hand tip right, X coordinate at different time laps. . . Right arm vs Left arm recordings . #3 represents the &#39;Not clear&#39; Command (see picture here above) . . What is the goal of the time series classification? . Let&#39;s we ask our operator to execute the command #4: Spread wings and collect the sensors data (24 channels) and save as sample. We then . Our goal is train our Neural Network model with many examples (in our case, we will see that we 360 samples), and then when we feed our model a sensor data without knowing to which command (fro example #2: All clear) it corresponds, our model will able to predict that this sensor data correspond to the #2: All clear command. . (of say #4: Spread wings) in order to be able to tell us that . The goal is to provide our Neural Network model data of 24 channels (for a given sample : co) . Downloading data . path = unzip_data(URLs_TS.NATOPS) . As you may noticed, the dataset has been downlaoded, unzipped, and stored in a new folder NATOPS in default fastai data folder in one single line. There are many .arff and .ts files. Both arff and ts are simply txt files that either store individual channel data (like NATOPSDimension12_TRAIN.arff which contain data from the 12 channel called dimension) or all channels (NATOPS_TRAIN.arff which contain all the 24 channels data in same file). Our dataset is split in TRAIN and TEST files. . Note: arff and ts are just ASCII data format used to store timeseries data. ts format is both more compact and contains more metadata. You can open these files in any text editor and explore them . Using Datasets class . tfms = [[ItemGetter(0), ToTensorTS()], [ItemGetter(1), Categorize()]] # Create a dataset ds = Datasets(items, tfms, splits=splits) ax = show_at(ds, 2, figsize=(1,1)) . Creating a Dataloaders object . bs = 32 tfm_norm = Normalize(scale_subtype = &#39;per_sample_per_channel&#39;, scale_range=(0, 1)) dls1 = ds.dataloaders(bs=bs, val_bs=bs * 2, after_batch=[tfm_norm]) . dls1.show_batch(max_n=9, chs=range(0,12,3)) . . Training a Model . # Number of channels c_in = get_n_channels(dls2.train) # 24 for NATOPS dataset # Number of classes c_out= dls2.c # 6 for NATOPS dataset . Create the InceptioTime model . model = inception_time(c_in, c_out).to(device=default_device()) . Creating a Learner object . learn = Learner(dls2, model, opt_func=Ranger, loss_func=LabelSmoothingCrossEntropy(), metrics=accuracy) . Finding optimal leaninig rate . lr_min, lr_steep = learn.lr_find() lr_min, lr_steep . Training model . learn.fit_one_cycle(30, lr_max=lr_steep) . Ploting the loss function . . Showing the results . learn.show_results(max_n=9, chs=range(0,12,3)) . . Showing the confusion matrix . interp = ClassificationInterpretation.from_learner(learn) interp.plot_confusion_matrix() . . Appendix A: Installation . One straightfoward and convenient way to install timeseries package is to install the 3 necessary packages from their corresponding github repositories. . # Install the latest version of fastai shared on github !pip install git+https://github.com/fastai/fastai2.git # Install the latest version of fastcore shared on github !pip install git+https://github.com/fastai/fastcore.git # Install the latest version of timeseries shared on github !pip install git+https://github.com/ai-fast-track/timeseries.git . . Note: One can also install the editable version of the 3 packages. Check the documentation . Conclusion . I hope this first article convey you into trying the timesrie` package. If you give it a try and found it interesting/helpful, please let others know it by staring it on github, and share with your friends and colleagues who might be interested in this topic. . Until the next post, I wish everybody well, and I hope you and your family, friends, loved ones will stay safe from this COVID-19 threat. I hope that something good will come out of this disruptive situation. As a positive outcome, we already have seen how more and more people are reaching out to each others. I hope the fastai community will use this adversity to build something even greater. I can already see this impact in growing online user group with video-conferences which will have a positive impact in sharing and spreading knowledge and ultimately democratizing further AI and Deep Learning. . Appendix B: Existing packages . sktime is a new scikit-learn compatible Python library for time series using machine learning techniques. It provides a uniﬁed interface for several time series learning tasks (univariate / multivariate classification, forecasting). . documentation . It also features a Deep Learning extension called sktime-dl sktime-dl is written using Keras. Presently, classification models are based on NN models found in dl-4-tsc. The latter is a library is open sourced by Hassan Fawaz . .",
            "url": "https://ai-fast-track.github.io/blog/timeseries/2020/03/14/mytest.html",
            "relUrl": "/timeseries/2020/03/14/mytest.html",
            "date": " • Mar 14, 2020"
        }
        
    
  

  
  
      ,"page0": {
          "title": "",
          "content": "Example Markdown Post . Basic setup . Jekyll requires blog post files to be named according to the following format: . YEAR-MONTH-DAY-filename.md . Where YEAR is a four-digit number, MONTH and DAY are both two-digit numbers, and filename is whatever file name you choose, to remind yourself what this post is about. .md is the file extension for markdown files. . The first line of the file should start with a single hash character, then a space, then your title. This is how you create a “level 1 heading” in markdown. Then you can create level 2, 3, etc headings as you wish but repeating the hash character, such as you see in the line ## File names above. . Basic formatting . You can use italics, bold, code font text, and create links. Here’s a footnote 1. Here’s a horizontal rule: . . Lists . Here’s a list: . item 1 | item 2 | . And a numbered list: . item 1 | item 2 | Boxes and stuff . This is a quotation . . You can include alert boxes …and… . . You can include info boxes Images . . Code . You can format text and code per usual . General preformatted text: . # Do a thing do_thing() . Python code and output: . # Prints &#39;2&#39; print(1+1) . 2 . Formatting text as shell commands: . echo &quot;hello world&quot; ./some_script.sh --option &quot;value&quot; wget https://example.com/cat_photo1.png . Formatting text as YAML: . key: value - another_key: &quot;another value&quot; . Tables . Column 1 Column 2 . A thing | Another thing | . Tweetcards . Altair 4.0 is released! https://t.co/PCyrIOTcvvTry it with: pip install -U altairThe full list of changes is at https://t.co/roXmzcsT58 ...read on for some highlights. pic.twitter.com/vWJ0ZveKbZ . &mdash; Jake VanderPlas (@jakevdp) December 11, 2019 Footnotes . This is the footnote. &#8617; . |",
          "url": "https://ai-fast-track.github.io/blog/archives/2020-01-14-test-markdown-post.html",
          "relUrl": "/archives/2020-01-14-test-markdown-post.html",
          "date": ""
      }
      
  

  

  
      ,"page2": {
          "title": "About Me",
          "content": "Pasionate about AI / Deep Learning and their applications for good. Helping democratizing AI / Deep Learning Happy to share all what I learned with all people no matter their background Hanging around at the most open and most welcoming deep learning community: fastai Trying to contribute to fastai community as much as I can: Learning from them and giving back. Presently working in developing a timeseries package for fastai2 . Background . BSc in Physics | MSc in Biomedical Engineering | PhD in Biomedical Engineering | Postdoc in both Biomedical Engineering and Neurosciences | Creator of StatMap3D software : Statitical Brain Electrical Mapping Software | Creator of DataFinder : Multidisciplinary Clinical and Research Database | . This website is powered by fastpages 1. . a blogging platform that natively supports Jupyter notebooks in addition to other formats. &#8617; . |",
          "url": "https://ai-fast-track.github.io/blog/about/",
          "relUrl": "/about/",
          "date": ""
      }
      
  

  

  
  

  

  
  

  

  
  

  
  

}